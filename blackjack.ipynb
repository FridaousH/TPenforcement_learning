{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning with Gymnasium ( Blackjack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "# Let's start by creating the blackjack environment.\n",
    "# Note: We are going to follow the rules from Sutton & Barto.\n",
    "# Other versions of the game can be found below for you to experiment.\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\", sab=True,render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the environment to get the first observation\n",
    "done = False\n",
    "observation, info = env.reset()\n",
    "\n",
    "#observation = (16,9,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fujitsu\\anaconda3\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:249: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# sample a random action from all valid actions\n",
    "action = env.action_space.sample()\n",
    "# action=1\n",
    "\n",
    "# execute the action in our environment and receive infos from the environment\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# observation=(24, 10, False)\n",
    "# reward=-1.0\n",
    "# terminated=True\n",
    "# truncated=False\n",
    "# info={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "discount_factor = 0.95\n",
    "learning_rate = 0.01\n",
    "n_episodes = 1000\n",
    "epsilon = 1.0\n",
    "epsilon_decay = epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "training_error = []\n",
    "observation, info = env.reset()\n",
    "# Défini l'action suivante\n",
    "\n",
    "def get_action(obs):\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(q_values[obs]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm SARSA or SARSA(0) se base sur l'équation ci-après:\n",
    "\n",
    "\n",
    "    target = reward + (gamma * Q[next_state][next_action])               # construct TD target\n",
    "    new_value = Q[state][action] + (alpha * (target - current))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mise à jour de la valeur de q par l'algorithm du td learning\n",
    "\n",
    "def update(\n",
    "        obs,\n",
    "        action,\n",
    "        reward,\n",
    "        terminated,\n",
    "        next_obs,\n",
    "    ):\n",
    "    \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "    future_q_value = (not terminated) * (q_values[next_obs][action])\n",
    "    temporal_difference = (\n",
    "            reward + discount_factor * future_q_value - q_values[obs][action]\n",
    "        )\n",
    "\n",
    "    q_values[obs][action] = (\n",
    "            q_values[obs][action] + learning_rate * temporal_difference\n",
    "        )\n",
    "    return q_values[obs][action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "n_episodes = 100\n",
    "\n",
    "final_epsilon = 0.1\n",
    "\n",
    "## Mise à jour de l'episode\n",
    "def decay_epsilon(epsilon,episode):\n",
    "    epsilon_decay = epsilon / (episode / 2)  # reduce the exploration over time\n",
    "    epsilon = max(final_epsilon,epsilon - epsilon_decay)\n",
    "    return epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 144.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    clear_output()\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        # choix de la nouvelle action\n",
    "        action = get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update de la q-value\n",
    "        q_values[obs][action] = update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        ## Visualisation de l'environnement\n",
    "        frame = env.render()\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "        decay_epsilon(1.0, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
